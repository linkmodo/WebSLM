<!doctype html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <title>Crazy Bananas SLM GPT</title>
  <link rel="manifest" href="./manifest.json">
  <link rel="icon" href="./public/logo.png">
  <link rel="apple-touch-icon" href="./public/logo.png">
  <meta name="theme-color" content="#0f172a">
  <link rel="stylesheet" href="./styles.css">
</head>
<body>
  <div class="app">
    <header>
      <div class="title">
        <img src="./public/logo.png" alt="Crazy Bananas SLM GPT" style="height: 150px; width: auto;">
        <div class="title-text">
          <h1>Crazy Bananas SLM GPT</h1>
          <p class="subtitle">Small Language Model GPTs that run entirely on your desktop browser.</p>
        </div>
      </div>
      <div class="actions">
        <button id="btn-settings" title="Settings">‚öô</button>
        <button id="btn-clear" title="Clear chat">üßπ</button>
      </div>
    </header>

    <section id="status" class="status">
      <span id="runtime-badge" class="badge">Detecting runtime‚Ä¶</span>
      <span id="init-label">Waiting‚Ä¶</span>
    </section>

    <main class="chat">
      <div id="messages" class="messages"></div>
      <form id="chat-form" class="input-row">
        <input id="prompt" type="text" placeholder="Ask anything (runs locally)..." autocomplete="off" />
        <input id="file-input" type="file" accept=".txt,.md,.json,.csv,.xml,.html,.js,.py,.cpp,.c,.java,.ts,.jsx,.tsx,.vue,.php,.rb,.go,.rs,.sh,.yml,.yaml,.toml,.ini,.cfg,.log,.pdf,.docx,.doc,.rtf,.odt,.png,.jpg,.jpeg,.gif,.bmp,.webp,.svg" multiple style="display: none;" />
        <button id="file-btn" type="button" title="Upload files">üìé</button>
        <button id="send" type="submit">Send ‚ñ∏</button>
        <button id="stop-btn" type="button" title="Stop generation" style="display: none; background: #ef4444; color: white;">‚èπ Stop</button>
      </form>
      <div id="file-preview" class="file-preview" style="display: none;"></div>
    </main>

    <dialog id="settings">
      <form method="dialog" class="settings">
        <h3>Settings</h3>
        <label title="Select the AI model to use. Larger models are more capable but require more resources.">Model (WebLLM)
          <select id="model-select">
            <option value="" data-desc="Please select a model to get started. Models will be downloaded on first use.">-- Select a Model --</option>
            <!-- Ultra Small Models (< 1B) -->
            <option value="SmolLM2-135M-Instruct-q0f16-MLC" data-desc="SmolLM2 135M (135M parameters) - Ultra-fast tiny model for basic tasks and low-resource devices. Minimal VRAM usage (~360MB).">SmolLM2 135M (Ultra Fast)</option>
            <option value="SmolLM2-360M-Instruct-q4f16_1-MLC" data-desc="SmolLM2 360M (360M parameters) - Very fast small model for simple Q&A and basic tasks. Low VRAM usage (~376MB).">SmolLM2 360M (Very Fast)</option>
            <!-- Small Models (1-2B) -->
            <option value="Llama-3.2-1B-Instruct-q4f16_1-MLC" data-desc="Llama 3.2 1B (1B parameters) - Meta's latest ultra-compact model with excellent efficiency. Good for basic tasks (~879MB VRAM).">Llama 3.2 1B (Efficient)</option>
            <option value="Qwen2.5-0.5B-Instruct-q4f16_1-MLC" data-desc="Qwen2.5 0.5B (0.5B parameters) - Alibaba's tiny but capable model. Excellent for quick responses (~945MB VRAM).">Qwen2.5 0.5B (Quick)</option>
            <option value="Qwen2.5-1.5B-Instruct-q4f16_1-MLC" data-desc="Qwen2.5 1.5B (1.5B parameters) - Balanced small model with good performance for its size (~1.6GB VRAM).">Qwen2.5 1.5B (Balanced)</option>
            <option value="SmolLM2-1.7B-Instruct-q4f16_1-MLC" data-desc="SmolLM2 1.7B (1.7B parameters) - HuggingFace's efficient small model with good reasoning capabilities (~1.8GB VRAM).">SmolLM2 1.7B (Reasoning)</option>
            <option value="stablelm-2-zephyr-1_6b-q4f16_1-MLC-1k" data-desc="StableLM Zephyr 1.6B (1.6B parameters) - Stability AI's chat-optimized model with 1K context (~1.5GB VRAM).">StableLM Zephyr 1.6B</option>
            <!-- Medium Models (2-4B) -->
            <option value="gemma-2-2b-it-q4f16_1-MLC-1k" data-desc="Gemma 2 2B (2B parameters) - Google's latest efficient model with strong performance (~1.6GB VRAM, 1K context).">Gemma 2 2B (Google)</option>
            <option value="Qwen2.5-3B-Instruct-q4f16_1-MLC" data-desc="Qwen2.5 3B (3B parameters) - Alibaba's mid-size model with excellent capabilities (~2.5GB VRAM).">Qwen2.5 3B (Capable)</option>
            <option value="Phi-3.5-mini-instruct-q4f16_1-MLC-1k" data-desc="Phi-3.5 Mini (3.8B parameters) - Microsoft's latest small model with excellent reasoning and coding capabilities (~2.5GB VRAM, 1K context).">Phi-3.5 Mini (Microsoft)</option>
            <!-- Large Models (7-8B) -->
            <option value="Mistral-7B-Instruct-v0.3-q4f16_1-MLC" data-desc="Mistral 7B v0.3 (7B parameters) - Strong performance in reasoning and instruction following. Good balance of capability and speed (~4.6GB VRAM).">Mistral 7B v0.3 (Recommended)</option>
            <option value="Llama-3.1-8B-Instruct-q4f16_1-MLC-1k" data-desc="Llama 3.1 8B (8B parameters) - Meta's powerful model with excellent instruction following (~4.6GB VRAM, 1K context).">Llama 3.1 8B (Most Capable)</option>
            <option value="Hermes-2-Pro-Llama-3-8B-q4f16_1-MLC" data-desc="Hermes 2 Pro (8B parameters) - Fine-tuned Llama 3 with function calling support. Excellent for complex tasks (~5GB VRAM).">Hermes 2 Pro (Function Calling)</option>
            <option value="Hermes-3-Llama-3.1-8B-q4f16_1-MLC" data-desc="Hermes 3 (8B parameters) - Latest Hermes model with advanced capabilities and function calling (~4.9GB VRAM).">Hermes 3 (Advanced)</option>
            <!-- Specialized Models -->
            <option value="Qwen2.5-Coder-1.5B-Instruct-q4f16_1-MLC" data-desc="Qwen2.5 Coder 1.5B (1.5B parameters) - Specialized for coding tasks with excellent programming capabilities (~1.6GB VRAM).">Qwen2.5 Coder 1.5B (Coding)</option>
            <option value="Qwen2.5-Coder-3B-Instruct-q4f16_1-MLC" data-desc="Qwen2.5 Coder 3B (3B parameters) - Advanced coding model with strong programming and debugging skills (~2.5GB VRAM).">Qwen2.5 Coder 3B (Advanced Coding)</option>
            <option value="Qwen2.5-Math-1.5B-Instruct-q4f16_1-MLC" data-desc="Qwen2.5 Math 1.5B (1.5B parameters) - Specialized for mathematical reasoning and problem solving (~1.6GB VRAM).">Qwen2.5 Math 1.5B (Mathematics)</option>
            <!-- DeepSeek Models -->
            <option value="DeepSeek-R1-Distill-Qwen-7B-q4f16_1-MLC" data-desc="DeepSeek R1 Distill Qwen 7B (7B parameters) - Latest DeepSeek reasoning model with advanced capabilities (~5.1GB VRAM).">DeepSeek R1 Qwen 7B (Reasoning)</option>
            <option value="DeepSeek-R1-Distill-Llama-8B-q4f16_1-MLC" data-desc="DeepSeek R1 Distill Llama 8B (8B parameters) - DeepSeek's reasoning model based on Llama architecture (~5GB VRAM).">DeepSeek R1 Llama 8B (Reasoning)</option>
          </select>
          <div id="model-description" class="model-description">
            Select a model to see details
          </div>
        </label>
        <label title="Set a fixed random seed for reproducible outputs. Same seed + same input = same output. 0 means random.">
          Seed 
          <input id="seed" type="number" min="0" step="1" value="0" />
        </label>
        <label title="Controls randomness: 0 = deterministic, higher values = more creative/random">
          Temperature 
          <input id="temperature" type="number" min="0" max="2" step="0.1" value="0.7" />
        </label>
        <div class="row">
          <button id="btn-reload-model" value="default" title="Apply settings and reload the AI model with current configuration">Reload model</button>
          <button id="btn-close-settings">Close</button>
        </div>
      </form>
    </dialog>
    <footer class="footer">
      <div class="footer-content">
        <p>Built by Li Fan</p>
        <p>
          <a href="https://github.com/linkmodo/WebSLM" target="_blank" rel="noopener noreferrer">
            View on GitHub ‚Üó
          </a>
        </p>
      </div>
    </footer>
  </div>

  <script type="module" src="./app.js"></script>
  <script>
    if ('serviceWorker' in navigator) {
      window.addEventListener('load', () => {
        navigator.serviceWorker.register('./sw.js').catch(console.error);
      });
    }
  </script>
</body>
</html>
